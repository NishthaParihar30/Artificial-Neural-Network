# -*- coding: utf-8 -*-
"""ann9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YMaOgfefHlkR_4NvObdnBs1HpKK0rdiN
"""

import numpy as np
import matplotlib.pyplot as plt

class LVQ:
    def __init__(self, learning_rate=0.1, epochs=100):
        """
        Initialize Learning Vector Quantization

        Args:
            learning_rate: Learning rate for weight updates
            epochs: Number of training epochs
        """
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.prototypes = None
        self.prototype_labels = None

    def fit(self, X, y, n_prototypes_per_class=1):
        """
        Train the LVQ model

        Args:
            X: Training data of shape (n_samples, n_features)
            y: Training labels of shape (n_samples,)
            n_prototypes_per_class: Number of prototypes per class
        """
        classes = np.unique(y)
        n_features = X.shape[1]

        self.prototypes = []
        self.prototype_labels = []

        # Initialize prototypes
        for cls in classes:
            idx = np.where(y == cls)[0]
            chosen = np.random.choice(idx, n_prototypes_per_class, replace=False)

            for i in chosen:
                self.prototypes.extend([X[i]])
                self.prototype_labels.append(cls)

        self.prototypes = np.array(self.prototypes)
        self.prototype_labels = np.array(self.prototype_labels)

        # Training loop
        for epoch in range(self.epochs):
            for x_i, label in zip(X, y):
                # Find closest prototype
                distances = np.linalg.norm(self.prototypes - x_i, axis=1)
                winner_idx = np.argmin(distances)

                # Update winner prototype
                if self.prototype_labels[winner_idx] == label:
                    # Move prototype closer if same class
                    self.prototypes[winner_idx] += self.learning_rate * (x_i - self.prototypes[winner_idx])
                else:
                    # Move prototype away if different class
                    self.prototypes[winner_idx] -= self.learning_rate * (x_i - self.prototypes[winner_idx])

            # Print progress
            if (epoch + 1) % 10 == 0:
                acc = self.score(X, y)
                print(f"Epoch {epoch + 1}/{self.epochs}, Accuracy: {acc:.2f}%")

    def predict(self, X):
        """
        Predict class labels for samples

        Args:
            X: Input data of shape (n_samples, n_features)

        Returns:
            predictions: Predicted class labels
        """
        preds = []
        for x in X:
            distances = np.linalg.norm(self.prototypes - x, axis=1)
            winner_idx = np.argmin(distances)
            preds.append(self.prototype_labels[winner_idx])

        return np.array(preds)

    def score(self, X, y):
        """
        Calculate accuracy

        Args:
            X: Input data
            y: True labels

        Returns:
            accuracy: Accuracy percentage
        """
        preds = self.predict(X)
        acc = np.mean(preds == y)
        return acc * 100


# Main execution
if __name__ == "__main__":
    # Set random seed for reproducibility
    np.random.seed(42)

    # Generate synthetic data for two classes
    class0 = np.random.randn(50, 2) + np.array([0, 0])
    class1 = np.random.randn(50, 2) + np.array([3, 3])

    X = np.vstack([class0, class1])
    y = np.array([0] * 50 + [1] * 50)

    # Create and train LVQ model
    lvq = LVQ(learning_rate=0.3, epochs=20)

    print("Training Learning Vector Quantization (LVQ)")
    print("=" * 50)
    lvq.fit(X, y, n_prototypes_per_class=2)
    print("=" * 50)
    print("Training complete!\n")

    # Make predictions
    preds = lvq.predict(X)
    acc = lvq.score(X, y)

    print(f"Final Training Accuracy: {acc:.2f}%")
    print(f"\nNumber of prototypes: {len(lvq.prototypes)}")
    print(f"Prototype labels: {lvq.prototype_labels}")

    # Visualization
    plt.figure(figsize=(12, 5))

    # Plot 1: Data and Prototypes
    plt.subplot(1, 2, 1)
    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Class 0', alpha=0.6)
    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Class 1', alpha=0.6)

    # Plot prototypes
    for i, (proto, label) in enumerate(zip(lvq.prototypes, lvq.prototype_labels)):
        color = 'blue' if label == 0 else 'red'
        plt.scatter(proto[0], proto[1], c=color, marker='X', s=200,
                   edgecolors='black', linewidth=2, label=f'Prototype {i}' if i < 2 else '')

    plt.title('Learning Vector Quantization (LVQ)')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Plot 2: Decision Boundary
    plt.subplot(1, 2, 2)

    # Create mesh grid
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))

    # Predict on mesh
    Z = lvq.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Plot decision boundary
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Class 0', alpha=0.6, edgecolors='k')
    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Class 1', alpha=0.6, edgecolors='k')

    # Plot prototypes
    for proto, label in zip(lvq.prototypes, lvq.prototype_labels):
        color = 'blue' if label == 0 else 'red'
        plt.scatter(proto[0], proto[1], c=color, marker='X', s=200,
                   edgecolors='black', linewidth=2)

    plt.title('Decision Boundary')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()