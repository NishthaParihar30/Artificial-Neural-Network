{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+nQPW+cV/atSqjatoE+Sr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2aHNM1P0Ybr","outputId":"d43e6602-1e49-482e-b82d-1fc36879842e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter gate (AND/OR): AND\n","\n","Epoch 1\n","Input: [1 0 0], Target: [-1], Updated Weights: [-1.  0.  0.]\n","Input: [1 0 1], Target: [-1], Updated Weights: [-2.  0. -1.]\n","Input: [1 1 0], Target: [-1], Updated Weights: [-3. -1. -1.]\n","Input: [1 1 1], Target: [1], Updated Weights: [-2.  0.  0.]\n","\n","Epoch 2\n","Input: [1 0 0], Target: [-1], Updated Weights: [-3.  0.  0.]\n","Input: [1 0 1], Target: [-1], Updated Weights: [-4.  0. -1.]\n","Input: [1 1 0], Target: [-1], Updated Weights: [-5. -1. -1.]\n","Input: [1 1 1], Target: [1], Updated Weights: [-4.  0.  0.]\n","\n","Epoch 3\n","Input: [1 0 0], Target: [-1], Updated Weights: [-5.  0.  0.]\n","Input: [1 0 1], Target: [-1], Updated Weights: [-6.  0. -1.]\n","Input: [1 1 0], Target: [-1], Updated Weights: [-7. -1. -1.]\n","Input: [1 1 1], Target: [1], Updated Weights: [-6.  0.  0.]\n","\n","Epoch 4\n","Input: [1 0 0], Target: [-1], Updated Weights: [-7.  0.  0.]\n","Input: [1 0 1], Target: [-1], Updated Weights: [-8.  0. -1.]\n","Input: [1 1 0], Target: [-1], Updated Weights: [-9. -1. -1.]\n","Input: [1 1 1], Target: [1], Updated Weights: [-8.  0.  0.]\n","\n","Epoch 5\n","Input: [1 0 0], Target: [-1], Updated Weights: [-9.  0.  0.]\n","Input: [1 0 1], Target: [-1], Updated Weights: [-10.   0.  -1.]\n","Input: [1 1 0], Target: [-1], Updated Weights: [-11.  -1.  -1.]\n","Input: [1 1 1], Target: [1], Updated Weights: [-10.   0.   0.]\n","Predictions after epoch 5: [0 0 0 0] (total=0)\n","\n","\n","Final Weights: [-10.   0.   0.]\n","\n","==================================================\n","BACKPROPAGATION IMPLEMENTATION\n","==================================================\n","\n"]}],"source":["import numpy as np\n","\n","def bipolar(x):\n","    \"\"\"Convert binary to bipolar representation\"\"\"\n","    return np.where(x == 0, -1, 1)\n","\n","def sigmoid(x):\n","    \"\"\"Sigmoid activation function\"\"\"\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    \"\"\"Derivative of sigmoid function\"\"\"\n","    return x * (1 - x)\n","\n","def predict_weights(x, weights):\n","    \"\"\"Predict output using weights\"\"\"\n","    # Calculate the weighted sum\n","    weighted_sum = np.dot(x, weights)\n","    # Apply the activation function (sign function for bipolar output)\n","    return np.where(weighted_sum >= 0, 1, -1)\n","\n","\n","def train_hebb_verbose(X, y, b=0.1, epochs=5):\n","    \"\"\"Train using Hebbian learning rule with verbose output\"\"\"\n","    w = np.zeros(X.shape[1])\n","\n","    for epoch in range(1, epochs+1):\n","        print(f\"\\nEpoch {epoch}\")\n","        for xi, yi in zip(X, y):\n","            w = w + yi * xi\n","            print(f\"Input: {xi}, Target: {yi}, Updated Weights: {w}\")\n","\n","    preds = predict_weights(X, w) # Pass the weights to predict_weights\n","    # Convert bipolar predictions to binary (0 and 1)\n","    preds_bi = np.where(preds == -1, 0, preds)\n","    print(f\"Predictions after epoch {epochs}: {preds_bi} (total={np.sum(preds_bi)})\")\n","\n","    return w # Return the final weights\n","\n","# Dataset\n","X = np.array([\n","    [1, 0, 0],\n","    [1, 0, 1],\n","    [1, 1, 0],\n","    [1, 1, 1]\n","])\n","\n","choice = input(\"Enter gate (AND/OR): \").strip().upper()\n","\n","if choice == \"AND\":\n","    targets = np.array([[0], [0], [0], [1]])\n","elif choice == \"OR\":\n","    targets = np.array([[0], [1], [1], [1]])\n","else:\n","    print(\"Invalid choice! Please enter AND or OR.\")\n","    exit()\n","\n","# Convert to bipolar\n","y = bipolar(targets)\n","\n","# Train using Hebbian learning\n","final_w = train_hebb_verbose(X, y, b=0.2, epochs=5)\n","\n","print(\"\\n\\nFinal Weights:\", final_w)\n","\n","# Alternative implementation using backpropagation (from second page)\n","print(\"\\n\" + \"=\"*50)\n","print(\"BACKPROPAGATION IMPLEMENTATION\")\n","print(\"=\"*50 + \"\\n\")\n","\n","def sigmoid_bp(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative_bp(x):\n","    return x * (1 - x)\n","\n","X_bp = np.array([\n","    [0, 0],\n","    [0, 1],\n","    [1, 0],\n","    [1, 1]\n","])\n","\n","gate = input(\"Enter gate (AND / OR / XOR): \").strip().upper()\n","\n","if gate == \"AND\":\n","    y = np.array([[0], [0], [0], [1]])\n","elif gate == \"OR\":\n","    y = np.array([[0], [1], [1], [1]])\n","elif gate == \"XOR\":\n","    y = np.array([[0], [1], [1], [0]])\n","else:\n","    print(\"Invalid gate! Defaulting to XOR.\")\n","    y = np.array([[0], [1], [1], [0]])\n","\n","np.random.seed(42)\n","input_layer_neurons = X_bp.shape[1]\n","hidden_layer_neurons = 2\n","output_neurons = 1\n","\n","# Initialize weights and biases\n","W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n","b1 = np.random.uniform(size=(1, hidden_layer_neurons))\n","W2 = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n","b2 = np.random.uniform(size=(1, output_neurons))\n","\n","epochs = 10000\n","learning_rate = 0.1\n","\n","for epoch in range(epochs):\n","    # Forward propagation\n","    hidden_input = np.dot(X_bp, W1) + b1\n","    hidden_output = sigmoid_bp(hidden_input)\n","\n","    final_input = np.dot(hidden_output, W2) + b2\n","    final_output = sigmoid_bp(final_input)\n","\n","    # Backpropagation\n","    error = y - final_output\n","    d_output = error * sigmoid_derivative_bp(final_output)\n","\n","    error_hidden = d_output.dot(W2.T)\n","    d_hidden = error_hidden * sigmoid_derivative_bp(hidden_output)\n","\n","    # Update weights and biases\n","    W2 += hidden_output.T.dot(d_output) * learning_rate\n","    b2 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n","\n","    W1 += X_bp.T.dot(d_hidden) * learning_rate\n","    b1 += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n","\n","    if epoch % 2000 == 0:\n","        print(f\"Epoch {epoch}, Loss: {np.mean(np.square(error))}\")\n","\n","print(f\"\\nFinal Predictions for {gate} gate:\")\n","print(np.round(final_output, 3))"]}]}